ENDPOINT_0:
  type: 'CHAT_OPENAI' # this specifies which type of langchain model to use
  url: null # change this with your own endpoint URL
  deployment_name: 'gpt-5'

  # we support different types of authentication to the endpoint.
  # 1. You can set the api_key as an environment variable, in the case of multiple endpoints, you can set the
  # api_key_env_var_name to the name of the environment variable.
  api_key_env_var_name: 'OPENAI_API_KEY'
  # 2. Alternatively, you can set the api_key as a secret in Azure KeyVault, update the keyvault_secret_name to the
  # name of the secret in Azure KeyVault.
  # keyvault_secret_name: "<your_secret_name>"
  # 3. If none of the above are set, we fall back to creating a token provider assuming you have the necessary azure
  # credentials set up.

  # If you have access to multiple endpoints, you can vary the speed_factor to control for the amount of data assigned
  # to each endpoint. This is used for efficient data sharing across multiple endpoints with different throughput.
  speed_factor: 1.0

  # Alternatively, you can set the num_parallel_processes to control the number of parallel processes that can be
  # spawned to handle requests to this endpoint. This is useful to parallelize requests to a single endpoint when
  # the endpoint has a high throughput. By default, all calls are sequential. This parameter enables parallelism.
  num_parallel_processes: 2
# If you have access to multiple endpoints, you can add more endpoints as follows. Change the speed factors as needed
# to control the amount of data assigned to each endpoint.
